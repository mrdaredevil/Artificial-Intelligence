\chapter{Playing Games}
One major breakthrough for the field of artificial intelligence happened in 1997 when the chess-playing computer
\href{https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)}{Deep Blue} was able to beat the World Chess
Champion \href{https://en.wikipedia.org/wiki/Garry_Kasparov}{Garry Kasparov} by $3\sfrac{1}{2}-2\sfrac{1}{2}$.
While \blue{Deep Blue} \index{deep blue} was based on special hardware, according to the
\href{http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html}{computer chess rating list} of the 8th
of February 2020, the chess program \href{https://en.wikipedia.org/wiki/Stockfish_(chess)}{Stockfish} runs
on ordinary desktop computers and has an \href{https://en.wikipedia.org/wiki/Elo_rating_system}{Elo rating} of 3495.  
To compare, according to the
\href{https://ratings.fide.com/top.phtml?list=men}{Fide} list of February 2020, the current 
World Chess Champion \href{https://en.wikipedia.org/wiki/Magnus_Carlsen}{Magnus Carlsen} has an Elo rating of
just 2862.  Hence, he wouldn't stand a chance to win a game against Stockfish.  In 2017, at the 
\href{https://en.wikipedia.org/wiki/Future_of_Go_Summit}{Future of Go Summit},  the computer program
\href{https://en.wikipedia.org/wiki/AlphaGo}{AlphaGo} \index{AlphaGo} was able to beat
\href{https://en.wikipedia.org/wiki/Ke_Jie}{Ke Jie}, \index{Ke Jie} 
who was at that time considered to be the best human
\href{https://en.wikipedia.org/wiki/Go_(game)}{Go} player in the world. 
Besides Go and chess, there are many other games where today the performance of a computer exceeds the
performance of human players.  To name just one more example, in 2019 the program
\href{https://en.wikipedia.org/wiki/Pluribus_(poker_bot)}{Pluribus} \index{Pluribus} was able to  
\href{https://arstechnica.com/science/2019/07/facebook-ai-pluribus-defeats-top-poker-professionals-in-6-player-texas-holdem/}{beat}
fifteen professional poker players in six-player no-limit 
\href{https://en.wikipedia.org/wiki/Texas_hold_%27em}{Texas Hold'em poker} resoundingly.

In this chapter we want to investigate how a computer can play a game.  To this end we define a
\blue{game} $\mathcal{G}$ as a six-tuple \index{game}
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{G} = \langle \texttt{States}, s_0, \texttt{Players}, \texttt{nextStates}, \texttt{finished},\texttt{utility} \rangle$
\\[0.2cm]
where the components are interpreted as follows:
\begin{enumerate}
\item $\texttt{States}$ is the set of all possible \blue{states} of the game.
\item $s_0 \in \texttt{States}$ is the \blue{start state}.
\item $\texttt{Players}$ is  the list of \blue{players} of the game.  The first element in \texttt{Players} is
      the player to start the game and after that the players take turns.  As we only consider \blue{two person}
      games, we assume that \texttt{Players} is a list of length two.  \index{two person games}
\item $\texttt{nextStates}$ is a function that takes a state $s \in \texttt{States}$ and a player $p \in \texttt{Players}$ and returns the set of
      states that can be reached if the player $p$ has to make a move in the state $s$.  Hence, the signature of
      $\texttt{nextStates}$ is given as follows:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{nextStates}: \texttt{States} \times \texttt{Players} \rightarrow 2^{\texttt{States}}$.
\item $\texttt{finished}$ is a function that takes a state $s$ and decides whether the games is finished.
      Therefore, the signature of $\texttt{finished}$ is
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{finished}: \texttt{States} \rightarrow \mathbb{B}$.
      \\[0.2cm]
      Here, $\mathbb{B}$ is the set of Boolean values, i.e.~we have $\mathbb{B} := \{ \texttt{true}, \texttt{false} \}$.
  
      Using the function $\texttt{finished}$, we define the set $\texttt{TerminalStates}$ as the set of those
      states such that the game has finished,  i.e.~we define \index{terminal state}
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{TerminalStates} := \{ s \in \texttt{States} \mid \texttt{finished}(s) \}$.
\item $\texttt{utility}$ is a function that takes a state $s \in \texttt{TerminalStates}$ and a player $p \in \texttt{Players}$.  It returns
      the \blue{value} that the game has for player $p$.  In general, a value is a real number,  but in all of
      our examples, this value will be an element from the set $\{-1, 0, +1\}$.  The value $-1$ indicates that
      player $p$ has lost the game, if the value is $+1$ the player $p$ has won the game, and if this value is
      $0$, then the game is a draw.  Hence the signature of $\texttt{utility}$ is
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{utility}: \texttt{TerminalStates} \times \texttt{Players} \rightarrow \{ -1, 0, +1\}$.
\end{enumerate}
In this chapter we will only consider so called \blue{two person, zero sum games}.  \index{zero sum game}
This means that the list $\texttt{Players}$ has exactly two elements.  If we call these players $\mathrm{A}$ and $\mathrm{B}$, i.e.~if we have
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{Players} = \bigl[ \mathrm{A}, \mathrm{B} \bigr]$,
\\[0.2cm]
then the game is called a \blue{zero sum game} iff we have
\\[0.2cm]
\hspace*{1.3cm}
$\forall s \in \texttt{TerminalStates}:\texttt{utility}(s, \texttt{A}) + \texttt{utility}(s, \texttt{B}) = 0$,
\\[0.2cm]
i.e.~the losses of player $\mathrm{A}$ are compensated by the wins of player $\texttt{B}$ and vice versa.
Games like \href{https://en.wikipedia.org/wiki/Go_(game)}{go} and 
\href{https://en.wikipedia.org/wiki/Chess}{chess} are two person, zero sum games.
We proceed to discuss an example.

\section{Tic-Tac-Toe}
The game \href{https://en.wikipedia.org/wiki/Tic-tac-toe}{tic-tac-toe} is played on a square board of size 
$3 \times 3$.  On every turn, one player puts an ``\texttt{X}'' on one of the free squares of the board, while
the other player puts an ``$\texttt{O}$'' onto a free square when it is his turn.  If the first player manages
to place three \texttt{X}s in a row, column, or diagonal, she has won the game.  Similarly, if the second
player manages to put three \texttt{O}s in a row, column, or diagonal, this player is the winner.  Otherwise,
the game is drawn.  \myFig{Tic-Tac-Toe.ipynb} shows a \textsl{Python} implementation of tic-tac-toe.
\index{tic-tac-toe}


\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    Players  = [ "X", "O" ]
    Start    = tuple( tuple(" " for col in range(3)) for row in range(3))
    to_list  = lambda State: [list(row) for row in State]
    to_tuple = lambda State: tuple(tuple(row) for row in State)
    
    def empty(S):
        return [ (row, col) for row in range(3)
                            for col in range(3)
                            if  S[row][col] == ' ' 
               ]
    
    def next_states(State, player):
        Empty  = empty(State)
        Result = []
        for row, col in Empty:
            NextState           = to_list(State)
            NextState[row][col] = player
            Result.append( to_tuple(NextState) )
        return Result
    
    All_Lines = [ [ (row, col) for col in range(3) ] for row in range(3) ] \
              + [ [ (row, col) for row in range(3) ] for col in range(3) ] \
              + [ [ (idx,   idx) for idx in range(3) ] ]                   \
              + [ [ (idx, 2-idx) for idx in range(3) ] ]
    
    def utility(State, player):
        for Pairs in All_Lines:
            Marks = { State[row][col] for row, col in Pairs }
            if len(Marks) == 1 and  Marks != { ' ' }: 
                if Marks == { player }:
                    return  1
                else:
                    return -1
        for row in range(3):
            for col in range(3):
                if State[row][col] == ' ':
                    return None   
        return 0            
    
    finished = lambda State: utility(State, "X") != None
\end{minted}
\vspace*{-1.0cm}
\caption{A \textsl{Python} implementation of tic-tac-toe.}
\label{fig:Tic-Tac-Toe.ipynb}
\end{figure}
 

\begin{enumerate}
\item The variable $\texttt{Players}$ stores the list of players.  Traditionally, the players in
      tic-tac-toe are called ``\texttt{X}'' and ``\texttt{O}''.  
\item The variable $\texttt{Start}$ stores the start state, which is an empty board.
      States are represented as tuples of tuples.  If $S$ is a state and $r,c \in \{0,1,2\}$,
      then $S[r][c]$ is the mark in row $r$ and column $c$.
      To represent states we have to use immutable data types, i.e.~tuples instead of lists, as we need to
      store states in sets later.  The entries in the inner tuples are the characters 
      ``\texttt{X}'', ``\texttt{O}'', and the blank character ``\texttt{ }''.
      As the state  $\texttt{Start}$ is the empty board, it is represented as a tuple of three tuples
      containing three blanks each:
      \begin{Verbatim}
      ( (' ', ' ', ' '), 
        (' ', ' ', ' '), 
        (' ', ' ', ' ')
      ).     
      \end{Verbatim}
\item As we need to manipulate States, we need a function that converts them into lists of lists.
      This function is called \texttt{to\_list}.
\item We also need to convert the lists of lists back into tuples of tuples.  This is achieved by the function
      \texttt{to\_tuple}.
\item Given a state $S$ the function $\mathtt{empty}(S)$ returns the list of pairs 
      $(\mathtt{row}, \mathtt{col})$ such that $S[\mathtt{row}][\mathtt{col}]$ is a blank character.  These pairs are
      the coordinates of the fields on the board $S$ that are not yet occupied by either an \texttt{"X"} or an
      \texttt{"O"}.   
\item The function $\texttt{next\_states}$ takes a $\texttt{State}$ and a $\texttt{player}$ and computes the list
      of states that can be reached from $\texttt{State}$ if $\texttt{player}$ is to move next.
      To this end, it first computes the set of \blue{empty} positions, i.e. those positions that have not yet
      been marked by either player. Every position is represented as pair of the
      form $(\texttt{row}, \texttt{col})$ where $\texttt{row}$ specifies the row and $\texttt{col}$ specifies
      the column of the position.  The position $(\texttt{row}, \texttt{col})$ is \blue{empty} in
      $\texttt{State}$ iff
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{State}[\texttt{row}][\texttt{col}] = \texttt{\symbol{34}\;\;\symbol{34}}$.
      \\[0.2cm]
      The computation of the empty position has been sourced out to the function $\texttt{empty}$.
      The function $\texttt{nextStates}$ then iterates over these empty positions. For every 
      empty position $(\texttt{row}, \texttt{col})$ it creates a new state $\texttt{NextState}$ that results
      from the current $\texttt{State}$ by putting the mark of $\texttt{player}$ in this position.  
      The resulting states are collected in the list $\texttt{Result}$ and returned.

      Note that we had to turn the \texttt{State} into a list of list in order to manipulate it.
      The manipulated State is then cast into a tuple of tuples.
\item The function $\texttt{utility}$ takes a $\texttt{State}$ and a $\texttt{player}$ as arguments.  If the game is 
      finished in the given $\texttt{State}$, it returns the value that this $\texttt{State}$ has for the
      current $\texttt{player}$.  If the outcome of the game is not yet decided, the value $\mathtt{None}$
      is returned instead. 
 
      In order to achieve its goal, the procedure first computes the set of all sets of coordinate pairs that 
      either specify a horizontal, vertical, or diagonal line on a $3 \times 3$ tic-tac-toe board.  Concretely,
      the variable \texttt{All\_Lines} has the following value:
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}{ll}
       \Bigl[ & \bigl[(1, 1), (1, 2), (1, 3)\bigr], \;
                \bigl[(2, 1), (2, 2), (2, 3)\bigr], \;
                \bigl[(3, 1), (3, 2), (3, 3)\bigr],   \\[0.1cm]
              & \bigl[(1, 1), (2, 1), (3, 1)\bigr], \;
                \bigl[(1, 2), (2, 2), (3, 2)\bigr], \;
                \bigl[(1, 3), (2, 3), (3, 3)\bigr],   \\[0.1cm]
              & \bigl[(1, 1), (2, 2), (3, 3)\bigr], \;
                \bigl[(3, 1), (2, 2), (1, 3)\bigr]    \\
       \Bigr]
      \end{array}
      $
      \\[0.2cm]
      The first line in this expression gives the set of pairs defining the rows, the second line defines 
      the columns, and the last line yields the tow diagonals.  Given a state $\texttt{State}$ and a set
      $\texttt{Pairs}$, the set 
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{Marks = \{ State[row][col] : [row, col] in Pairs \}}
      \\[0.2cm]
      is the set of all marks in the line specified by $\texttt{Pairs}$.  For example, if 
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{Pairs = \{ [1, 1], [2, 2], [3, 3] \}},
      \\[0.2cm]
      then $\texttt{Marks}$ is the set of marks on the falling diagonal.
      The game is decided if all entries in a set of the form 
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{Marks := \{ State[row][col] : [row, col] in Pairs \}}
      \\[0.2cm]
      where \texttt{Pairs} is a list from \texttt{all\_lines} either have the value
      ``$\texttt{X}$'' or the value ``$\texttt{O}$''.  In this case, the set \texttt{Marks} has exactly one
      element which is different from the blank.  If this element is the same as $\texttt{player}$, then the
      game is \blue{won} by $\texttt{player}$, otherwise it must be the mark of his opponent and hence the game
      is \blue{lost} for him. 

      If there are any empty squares on the board but the game has not yet been decided,
      then the function returns \texttt{None}.  Finally, if there are no more empty squares left, the game is a
      \blue{draw}. 
\item The function $\texttt{finished}$ takes a $\texttt{State}$ and checks whether the game is finished.
      To this end it computes the $\texttt{utility}$ of the state for the player ``$\texttt{X}$''.  
      If this $\texttt{utility}$ is different from $\mathtt{None}$, then game is finished.  Note that it does make no
      difference whether we take the utility of the state for the player ``$\texttt{X}$'' or for the player
      ``$\texttt{O}$'': If the game is finished for  ``$\texttt{X}$'', then it is also finished for ``$\texttt{O}$'' and vice versa.
\end{enumerate}

\section{The Minimax Algorithm \label{sec:minimax}}
\index{minimax algorithm}
Having defined the notion of a game, our next task is to come up with an algorithm that can play a game.  The
algorithm that is easiest to explain is the \href{https://en.wikipedia.org/wiki/Minimax}{minimax algorithm}.  This
algorithm is based on the notion of the \blue{value} of a state.  Conceptually, the notion of the \emph{value}
of a state is an extension of the notion of the \emph{utility}.  While the utility is only defined for terminal
states, the value is defined for all states.  Formally, we define a function
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{value}: \texttt{States} \times \texttt{Players} \rightarrow \{-1, 0, +1\}$
\\[0.2cm]
that takes a state $s \in \texttt{States}$ and a player $p \in \texttt{Players}$ and returns the value of $s$ provided both the player $p$ and his
opponent play \blue{optimally}.  The easiest way to define this function is via recursion.  As the
\texttt{value} function is an extension of the \texttt{utility} function, The base case is as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{finished}(s) \rightarrow \texttt{value}(s, p) = \texttt{utility}(s, p)$. \hspace*{\fill} (1)
\\[0.2cm]
If the game is not yet finished, assume that player $o$ is the opponent of player $p$.  Then we define
\\[0.2cm]
\hspace*{1.3cm}
$\neg \texttt{finished}(s) \rightarrow 
 \texttt{value}(s, p) = \max\bigl(\bigl\{
                     -\texttt{value}(n, o) \mid n \in \texttt{nextStates}(s, p)
                     \bigr\}\bigr)
$.  \hspace*{\fill} (2)
\\[0.2cm]
The reason is that, if the game is not finished yet, the player $p$ has to evaluate all possible moves.  
From these, the player $p$ will choose the move that maximizes the value of the game for herself.  In order to
do so, the player $p$ computes the set 
$\texttt{nextStates}(s, p)$ of all states that can be reached from the state $s$ in any one move of the player $p$.
Now if $n$ is a state that results from player $p$ making some move, then in state $n$ it is the turn of the other player
$o$ to make a move.  Hence, in order to evaluate the state $n$, we have to call the function $\texttt{value}$
recursively as $\texttt{value}(n,o)$.   Since the gains of the other player $o$ are the losses of the player
$p$, we have to take the negative of  $\texttt{value}(n, o)$.
\myFig{Minimax.ipynb} shows an implementation of this strategy.


\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.0cm,
                xrightmargin  = 0.0cm,
              ]{python3}
    other = lambda p: [o for o in Players if o != p][0]
  
    Cache = {}

    def memoize(f):
        global Cache

        def f_memoized(*args):
            if args in Cache:
                return Cache[args]
            result = f(*args)
            Cache[args] = result
            return result

        return f_memoized
  
    @memoize    
    def value(State, player):
        if finished(State): return utility(State, player)
        return max([-value(ns, other(player)) for ns in next_states(State, player)])
    
    def best_move(State, player):
        NS        = next_states(State, player)
        bestVal   = value(State, player)
        BestMoves = [s for s in NS if -value(s, other(player)) == bestVal]
        BestState = random.choice(BestMoves)
        return bestVal, BestState

    def play_game(canvas):
        State = Start
        while (True):
            firstPlayer = Players[0]
            val, State  = best_move(State, firstPlayer);
            draw(State, canvas)
            if finished(State):
                final_msg(State)
                break
            State = get_move(State)
            draw(State, canvas)
            if finished(State):
                final_msg(State)
                break    
\end{minted}
\vspace*{-1.0cm}
\caption{The Minimax algorithm.}
\label{fig:Minimax.ipynb}
\end{figure}
\begin{enumerate}
\item Given a player $\texttt{p}$, the function $\texttt{other}$ computes the other player,
      which is the first element of the list $\texttt{Players}$ that is different from $p$.
      This works because we assume that there are just two players and these players are the two elements of
      the list $\texttt{Players}$ that has been defined in the notebook defining the game.
\item $\texttt{Cache}$ is a dictionary that is initially empty.  This dictionary is used as a memory cache by 
      the function $\texttt{memoize}$.
\item The function $\mathtt{memoize}$ is a second order function that takes a function $f$ as its argument.
      It creates a \blue{memoized} version of the function $f$:  This memoized version of $f$, which is called
      $\mathtt{f\_memoized}$, first tries to retrieve the value of $f$ from the dictionary $\texttt{Cache}$.
      If this is successful, the function returns the cached value.  Otherwise, the function $f$ is called
      to compute the result.  This result is then stored in the $\mathtt{Cache}$ before it is returned.
      The function $\mathtt{memoize}$ returns the memoized version of $f$.
      \index{memoization}
\item The implementation of the function $\texttt{value}$ implements the formulas $(1)$ and $(2)$ that were
      used to define the function $\mathtt{value}$ abstractly.
      However, note that we have preceded the definition of the function $\texttt{value}$ with the
      \blue{decorator} \texttt{@memoize}, which turns the function \texttt{value} into a memoized function.
      Hence, when the function $\texttt{value}$ is called a
      second time with the same pair of arguments, it does not recompute the value but rather the value is
      looked up from the variable $\texttt{Cache}$ that stores all previous results computed by the function $\texttt{value}$.  To
      understand why this is important, 
      let us consider how many states would be explored in the case of tic-tac-toe if we would not use the idea
      of memorizing previous results.  In this case, we have 9 moves for player
      \texttt{X} from the start state, then 8 moves for player \texttt{O}, then again 7 moves for
      player \texttt{O}.  If we disregard the fact that some games are decided after fewer than 9 moves,
      the function $\texttt{value}$ needs to consider 
      \\[0.2cm]
      \hspace*{1.3cm}
      $9 \cdot 8 \cdot 7 \cdot {\dots} \cdot 2 \cdot 1 = 9! = 362\,880$
      \\[0.2cm]
      moves.  However, if we count the number of possibilities of putting 5 ``\texttt{O}''s and 4
      ``\texttt{X}''s on a $3 \times 3$ board, we see that there are only
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds {9 \choose 5} = \frac{9!}{5! \cdot 4!} = 126$
      \\[0.2cm]
      possibilities, because we only have to count the number of ways to put 5 ``\texttt{O}''s on
      9 positions and that number is the same as the number of subsets of five elements from a set of nine elements.
      Therefore, if we disregard the fact that some games are decided after fewer than nine moves,  there are a
      factor of $5! \cdot 4! = 2880$ less terminal states to evaluate if we use memoization!

      As we have to evaluate not just terminal states but all states, the saving is actually a bit smaller that
      2880.  The next exercise explores this in more detail.
\item The function $\texttt{best\_move}$ takes a $\texttt{State}$ and a $\texttt{player}$ and returns a pair $(v,s)$
      where $s$ is a state that is optimal for the $\texttt{player}$ and such that $s$ can be reached in one step from
      $\texttt{State}$.  Furthermore, $v$ is the value of this state.
      \begin{enumerate}[(a)]
      \item To this end, it first computes the set $\texttt{NS}$ of all states that can be reached 
            from the given $\texttt{State}$ in one step if $\texttt{player}$ is to move next.
      \item $\texttt{bestValue}$ is the best value that $\texttt{player}$ can achieve in the given $\texttt{State}$.
      \item $\texttt{BestMoves}$ is the set of states that  $\texttt{player}$ can move to and that are optimal
            for her.
      \item The function returns randomly one of those states $\texttt{ns} \in \texttt{NS}$ such that 
            the value of $\texttt{ns}$ is optimal, i.e.~is equal to $\texttt{bestValue}$.
            We use randomization here since we want to have more interesting games.  If we would always choose
            the first state that achieves the best value, then our program would always make the same move in
            a given state.  Hence, playing the program would get boring much sooner.
      \end{enumerate}
\item The function $\texttt{play\_game}$ is used to play a game.
      \begin{enumerate}
      \item Initially, $\texttt{State}$ is the $\texttt{startState}$.
      \item As long as the game is not finished, the procedure keeps running.
      \item We assume that the computer goes first and therefore define  \texttt{firstPlayer} as the first
            element of the list $\texttt{Players}$.  Next, the function $\texttt{best\_move}$ is used to
            compute the state that results from the best move of $\texttt{firstPlayer}$.
            This resulting state is then shown.
      \item After that, it is checked whether the game is finished.
      \item If the game is not  yet finished, the user is asked to make its move via the function
            $\texttt{get\_move}$ that asks the user to enter a move.
            The state resulting from this move is then returned and displayed.

      \item Next, we have to check whether the game is finished after the  move of the user has been executed.
      \item The \texttt{while}-loop keeps iterating until the game is finished.
            We do not have to put a test into the condition of this \texttt{while}-loop as we call the function
            $\texttt{finished}(\texttt{State})$ every time that a new $\texttt{State}$ has been reached.
            If the game is finished, a message giving the result of the game is printed.
      \end{enumerate}
\end{enumerate}
In order to better understand the reason for using memoization in the implementation of the function
\texttt{value} we introduce the following notions.
\begin{Definition}[\blue{Game Tree}]
  Assume that
  \\[0.2cm]
  \hspace*{1.3cm}
  $\mathcal{G} = \langle \texttt{States}, s_0, \texttt{Players}, \texttt{nextStates}, \texttt{finished},\texttt{utility} \rangle$
  \\[0.2cm]
  is a game. Then a \blue{play of length $n$} is a list of states of the form 
  \\[0.2cm]
  \hspace*{1.3cm}
  $[s_0, s_1, \cdots, s_n]$ \quad such that \quad $\forall i\in\{0,\cdots,n-1\}: s_{i+1} \in \texttt{nextStates}(s_i, p_i)$,
  \\[0.2cm]
  where the players $p_i$ are defined such that for all even $i\in\{0,\cdots,n-1\}$ we have that $p_i$ is the
  first element in the list $\texttt{Players}$, while $p_i$ is the second element otherwise.
  The \blue{game tree} of the game $\mathcal{G}$ is the set of all possible plays.  \eoxs
\end{Definition}

\noindent
The following exercise shows why memoization is important.

\exercise
In \blue{simplified tic-tac-toe} the game only ends when there are no more empty squares left.
The player \texttt{X} wins if she has more rows, columns, or diagonals of three \texttt{X}s than the player
\texttt{O} has rows, columns, or diagonals of three \texttt{O}s.  Similarly, the player \texttt{O} wins
if he has more rows, columns, or diagonals of three \texttt{O}s than the player \texttt{X} has rows, columns,
or diagonals of three \texttt{X}s.  Otherwise, the game is a draw. 
\begin{enumerate}[(a)]
\item Derive a formula to compute the size of the game tree of simplified tic-tac-toe.
\item Write a short program to evaluate the formula derived in part (a) of this exercise.
\item Derive a formula that gives the number of all states of simplified tic-tac-toe.  

      \textbf{Notice} that this question does not ask for the number of all terminal states but rather asks for
      all states. 
\item Write a short program to evaluate the formula derived in part (c) of this exercise.
\end{enumerate}

\section{\href{https://en.wikipedia.org/wiki/Alpha-beta_pruning}{$\alpha$-$\beta$-Pruning}}
The efficiency of the minimax algorithm can be improved if we provide two additional arguments to the function
$\texttt{value}$.  Traditionally, these arguments are called $\blue{\alpha}$ and $\blue{\beta}$.  In order to be able to
distinguish between the old function $\texttt{value}$ and its improved version, we call the improved version 
$\texttt{alphaBeta}$.  The idea is that the function $\texttt{alphaBeta}$ and the function $\texttt{value}$ are
related by the following requirements: 
\begin{enumerate}
\item As long as $\texttt{value}(s, p)$ is between $\alpha$ and $\beta$, the function
      $\texttt{alphaBeta}$ computes the same result as the function $\texttt{value}$,
      i.e.~we have
      \\[0.2cm]
      \hspace*{0.3cm}
      $\alpha \leq \texttt{value}(s, p) \leq \beta \;\rightarrow\;
         \texttt{alphaBeta}(s, p, \alpha, \beta) = \texttt{value}(s,p)
      $.
\item If $\texttt{value}(s, p) < \alpha$, we require that the value returned by
      $\texttt{alphaBeta}$ is less than or equal to $\alpha$, i.e.~we have 
      \\[0.2cm]
      \hspace*{0.3cm}
      $\texttt{value}(s, p) < \alpha \;\rightarrow\; \texttt{alphaBeta}(s, p, \alpha, \beta) \leq \alpha$.
\item Similarly, if $\texttt{value}(s, p) > \beta$, we require that the value
      returned by $\texttt{valueAlphaBeta}$ is bigger than or equal to $\beta$, i.e.~we have 
      \\[0.2cm]
      \hspace*{0.3cm}
      $\beta < \texttt{value}(s, p) \;\rightarrow\; \beta \leq \texttt{alphaBeta}(s, p, \alpha, \beta)$.
\end{enumerate}
Therefore, $\texttt{alphaBeta}(\texttt{State}, \texttt{player})$  is only an \blue{approximation} of
$\texttt{value}(\texttt{State}, \texttt{player})$.  However, it turns out that this approximation is all that
is needed.  \myFig{Alpha-Beta-Pruning.ipynb:alphaBeta} shows an implementation of the function $\texttt{alphaBeta}$ that
satisfies the specification given above.  Once the function $\texttt{alphaBeta}$ is implemented, the function
$\texttt{value}$ can then be computed as 
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{value}(s, p ) := \texttt{alphaBeta}(s, p, -1, +1)$.
\\[0.2cm]
The reason is that we already know that $-1 \leq \texttt{value}(s,p) \leq +1$ and hence the first case of the
specification of $\texttt{alphaBeta}$ guarantees that the equation
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{value}(s,p) = \texttt{alphaBeta}(s,p,-1,+1)$
\\[0.2cm]
holds.  Since $\texttt{alphaBeta}$ is implemented as a recursive procedure, 
the fact that the implementation of $\texttt{alphaBeta}$ shown in \myFig{Alpha-Beta-Pruning.ipynb:alphaBeta} satisfies the
specification given above can be established by computational induction.  A proof by computational induction
can be found in an
\href{https://pdfs.semanticscholar.org/dce2/6118156e5bc287bca2465a62e75af39c7e85.pdf}{article} by Donald
E.~Knuth and Ronald W.~Moore \cite{knuth:1975}. 



\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.0cm,
                xrightmargin  = 0.0cm,
              ]{python3}
    def alphaBeta(State, player, alpha, beta):
        if finished(State):
            return utility(State, player)
        val = alpha
        for ns in next_states(State, player):
            val = max(val, -value(ns, other(player), -beta, -alpha))
            if val >= beta:
                return val
            alpha = max(val, alpha)
        return val
\end{minted}
\vspace*{-1.0cm}
\caption{$\alpha$-$\beta$-Pruning.}
\label{fig:Alpha-Beta-Pruning.ipynb:alphaBeta}
\end{figure}

\noindent
We proceed to discuss the implementation of the function $\texttt{alphaBeta}$, which is shown in
\myFig{Alpha-Beta-Pruning.ipynb:alphaBeta}.
\begin{enumerate}
\item If $\texttt{State}$ is a terminal state, the function returns the $\texttt{utility}$ of the given
      $\texttt{State}$ with respect to $\texttt{player}$.
\item The variable $\texttt{val}$ is supposed to store the maximum of the values of all states
      that can be reached from the given $\texttt{State}$ if $\texttt{player}$ makes one move.
      
      According to the specification of $\texttt{alphaBeta}$,  we are not interested in values that are less than
      $\texttt{alpha}$.  Hence, it suffices to initialize $\texttt{val}$ with $\texttt{alpha}$.   This way, in the case that we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{value}(\texttt{State},\texttt{player}) < \alpha$,
      \\[0.2cm]
      instead of returning the true value of the given $\texttt{State}$, the function
      $\texttt{alphaBeta}(\texttt{State},\texttt{player},\alpha,\beta)$ will instead return the value $\alpha$, which is permitted by its specification.
\item Next, we iterate over all successor states $\texttt{ns} \in \texttt{next\_states}(\texttt{State}, \texttt{player})$.
\item We have to recursively evaluate the states $\texttt{ns}$ with respect to the opponent  $\texttt{other}(\texttt{player})$.
      Since the value of a state for the opponent is the negative of the value for
      $\texttt{player}$, we have to exchange the roles of $\alpha$ and $\beta$ and prefix them with a negative
      sign.  Note that in the recursive call we do not call the function $\mathtt{alphaBeta}$ directly, but
      rather we call the function $\mathtt{value}$.  This function is a wrapper for the function
      $\mathtt{alphaBeta}$.  The purpose of this wrapper is to \blue{memoize} the function
      $\mathtt{alphaBeta}$.  This is more complicated now due to the presence of the parameters $\alpha$ and $\beta$.
      The details will be explained below when we discuss the function $\texttt{value}$ that is shown in Figure
      \ref{fig:Alpha-Beta-Pruning.ipynb:value}.
\item As the specification of $\texttt{alphaBeta}$ ask us to compute the value of $\texttt{State}$ only in
      those cases where it is less than or equal to $\beta$, once we find a successor state $s$ that has a
      value $\texttt{val}$ that is at least as big as $\beta$ we can \blue{stop any further evaluation} of the successor
      states and return the value $\texttt{val}$.

      \underline{In }p\underline{ractice},\underline{ this shortcut results in si}g\underline{nificant savin}g\underline{s of com}p\underline{utation time!}
\item Once we have found a successor state that has a value $\texttt{val}$ greater than $\texttt{alpha}$,
      we can increase $\texttt{alpha}$ to the value $\texttt{val}$.  The reason is, that once we know we can
      achieve a value of $\texttt{val}$ we are no longer interested in any values that are less than $\texttt{val}$.
      This is the reason for assigning to $\texttt{alpha}$ the maximum of $\texttt{val}$ and $\texttt{alpha}$.
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    def value(State, player, alpha=-1, beta=1):
        global Cache
        if State in Cache:
            val, a, b = Cache[State]
            if a <= alpha and beta <= b:
                return val
            else:
                alpha = min(alpha, a)
                beta  = max(beta , b)
                val   = alphaBeta(State, player, alpha, beta)
                Cache[State] = val, alpha, beta
                return val
        else:
            val = alphaBeta(State, player, alpha, beta)
            Cache[State] = val, alpha, beta
            return val                
\end{minted}
\vspace*{-1.0cm}
\caption{The function $\mathtt{value}$ that memoizes the function $\mathtt{alphaBeta}$.}
\label{fig:Alpha-Beta-Pruning.ipynb:value}
\end{figure}
The function $\texttt{value}$ shown in Figure \ref{fig:Alpha-Beta-Pruning.ipynb:value} is merely a wrapper of
the function $\mathtt{alphaBeta}$ that memoizes the results of $\mathtt{alphaBeta}$.  This is more complicated
now because of the parameters $\alpha$ and $\beta$.  Of course, we could just use the decorator
\texttt{\@memoize}.  The problem with this approach is that the function $\mathtt{alphaBeta}$ might be called
with the same value for its parameters $\mathtt{State}$ and $\mathtt{player}$, but different values for the
parameters $\mathtt{alpha}$ and $\mathtt{beta}$.  This would have two consequences:
\begin{enumerate}
\item The directory $\mathtt{Cache}$ would require much more memory as there are many more combinations of the parameters
      of the function $\texttt{alphaBeta}$ then there had been combinations for the function $\mathtt{value}$ when
      we had implemented the minimax algorithm.  
\item The directory $\texttt{Cache}$ would become much less useful than in our implementation of the minimax
      algorithm because we would have many cache misses.  
\end{enumerate}
The solution is to ensure that any $\mathtt{State}$ is stored at most once in $\mathtt{Cache}$.  But instead of storing just the
value, we also have to store the values of the parameters $\mathtt{alpha}$ and $\mathtt{beta}$ that have been
used to compute the value of the given $\mathtt{State}$, i.e.~we now have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{Cache}[\mathtt{State}] = (\mathtt{value}, \mathtt{alpha}, \mathtt{beta})$
\\[0.2cm]
if we have computed that
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{alphaBeta}(\mathtt{State}, \mathtt{player}, \mathtt{alpha}, \mathtt{beta}) = \mathtt{value}$.
\\[0.2cm]
It turns out that there is no need to store the argument $\mathtt{player}$ since in the games that we consider
this information can always be computed from the $\mathtt{State}$.

Now the crucial idea of our implementation the function $\mathtt{value}$ is the following:  If we ever want to
compute 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{alphaBeta}(\mathtt{State}, \mathtt{player}, \mathtt{alpha}, \mathtt{beta})$
\\[0.2cm]
and we have that 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{Cache}[\mathtt{State}] = (\mathtt{value}, a, b)$,
\\[0.2cm]
then we have to check whether the interval $[\mathtt{alpha}, \mathtt{beta}]$ is \blue{more general} than the
interval $[a,b]$, i.e.~we have to check that
\\[0.2cm]
\hspace*{1.3cm}
$a \leq \mathtt{alpha}$ \quad and \quad $\mathtt{\beta} \leq b$ 
\\[0.2cm]
holds.  In this case we know that indeed
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{alphaBeta}(\mathtt{State}, \mathtt{player}, \mathtt{alpha}, \mathtt{beta}) = \mathtt{value}$
\\[0.2cm]
and hence we can use the value stored in the $\mathtt{Cache}$.  If the inequations $a \leq \mathtt{alpha}$ and
$\mathtt{\beta} \leq b$  are not satisfied, then we compute
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{alphaBeta}(\mathtt{State}, \mathtt{player}, \min(\mathtt{alpha}, a), \max(\mathtt{beta}, b))$
\\[0.2cm]
and store the resulting value together with the new interval $[\min(\mathtt{alpha}, a), \max(\mathtt{beta},
b)]$ in the $\mathtt{Cache}$. 
This way, the value stored in the $\mathtt{Cache}$ is more general than the previous value stored and we can
expect more cache hits later.

\section{Depth Limited Search}
In practice, most games are far too complex to be evaluated completely, i.e.~the size of the set
$\texttt{States}$ is so big that even the fastest computer does not stand a chance to explore this set
completely.  For example, it is believed\footnote{
  For reference, compare the wikipedia article on the so-called
  \href{https://en.wikipedia.org/wiki/Shannon_number}{Shannon number}.
  The Shannon number estimates that there are at least $10^{120}$ different plays in chess.  However, the
  number of states is estimated to be about $10^{50}$. 
}
that in chess there are about $10^{50}$ different states that could occur in a game.
Hence, it is impossible to explore all possible states in chess.  Instead, we have to limit
the exploration in a way that is similar to the way professional players evaluate their game:  Usually, a
player considers all variations of the game for, say, the next three moves.  After a given number of moves, the
value of a position is estimated using an \blue{evaluation function}.  This function \blue{approximates} the true
value of a given state via a heuristic.

In order to implement this idea, we add a parameter $\texttt{limit}$ to the procedure $\texttt{alphaBeta}$ that
was shown in the previous section.  On
every recursive invocation of the function $\texttt{alphaBeta}$, the parameter $\texttt{limit}$ is decreased.
Once the limit reaches $0$, instead of invoking the function $\texttt{alphaBeta}$ again recursively, we try to
estimate the value of 
the given $\texttt{State}$ using our \blue{evaluation function}.  This leads to the code shown in
\myFig{Game.ipynb}. 


\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  bgcolor       = sepia,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]{python3}
    def alphaBeta(State, player, limit, heuristic, alpha=-1, beta=1):
        if finished(State):
            return utility(State, player)
        if limit == 0:
            return heuristic(State, player)
        val = alpha
        for ns in next_states(State, player):
            val_ns = value(ns, other(player), limit-1, heuristic, -beta, -alpha)
            val    = max(val, -val_ns)
            if val >= beta:
                return val
            alpha = max(val, alpha)
        return val
\end{minted}
\vspace*{-1.0cm}
\caption{Depth-limited $\alpha$-$\beta$-pruning.}
\label{fig:Game.ipynb}
\end{figure}
When we compare this Figure with \myFig{Alpha-Beta-Pruning.ipynb:alphaBeta}, the only difference is in line 4
where we test whether the $\mathtt{limit}$ is $0$.  In this case, instead of trying to recursively evaluate the
states reachable from $\mathtt{State}$, we evaluate the $\mathtt{State}$ with our $\mathtt{heuristic}$ function.
Later in the recursive calls of the function \texttt{value} we have to take care to decrease the parameter
$\mathtt{limit}$.

For a game like tic-tac-toe it is difficult to come up with a decent heuristic.  A very crude approach would be
to define:
\\[0.2cm]
\hspace*{1.3cm}
\texttt{heuristic := [State, player] |-> 0;}
\\[0.2cm]
This heuristic would simply estimate the value of all states to be $0$.  As this heuristic is only called after
it has been tested that the game has not yet been decided, this approach is not utterly unreasonable.  For a more
complex game like chess, the heuristic could instead be a \blue{weighted count} of all pieces.  Concretely, the
algorithm for estimating the value of a state would work as follows:
\begin{enumerate}
\item Initially, the variable $\texttt{sum}$ is set to $0$:
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{sum := 0;}
\item We would count the number of white rooks $\texttt{Rook}_{\mathrm{white}}$ and black rooks $\texttt{Rook}_{\mathrm{black}}$,
      subtract these numbers from each other and multiply the difference by $5$.  
      The resulting number would be added to $\texttt{sum}$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{sum} \;\texttt{+=}\; (\texttt{Rook}_{\mathrm{white}} - \texttt{Rook}_{\mathrm{black}}) \cdot 5\texttt{;}$
\item We would count the number of white bishops $\texttt{Bishop}_{\mathrm{white}}$ and black bishops
      $\texttt{Bishop}_{\mathrm{black}}$,
      subtract these numbers from each other and multiply the difference by $3$.  
      The resulting number would be added to $\texttt{sum}$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{sum} \;\texttt{+=}\; (\texttt{Bishop}_{\mathrm{white}} - \texttt{Bishop}_{\mathrm{black}}) \cdot 3\texttt{;}$
\item In a similar way we would count knights, queens, and pawns.  Approximately, the weights of
      knights are $3$, a queen is worth $9$ and a pawn is worth $1$.
\end{enumerate}
The resulting $\texttt{sum}$ can then be used as an approximation of the value of a state.
More details about the weights of the pieces can be found in the Wikipedia article 
``\href{https://en.wikipedia.org/wiki/Chess_piece_relative_value}{chess piece relative value}''.



\exercise
Read up on the game \href{https://en.wikipedia.org/wiki/Connect_Four}{Connect Four}.  You can play it online at
\\[0.2cm]
\hspace*{1.3cm}
\href{http://www.connectfour.org/connect-4-online.php}{\texttt{http://www.connectfour.org/connect-4-online.php}}
\\[0.2cm]
Your task is to implement this game.  On my github page (\texttt{https://github.com/karlstroetmann}) at
\\[0.2cm]
\hspace*{1.3cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/Connect-Four-Frame.ipynb}{Artificial-Intelligence/blob/master/Python/Connect-Four-Frame.ipynb} 
\\[0.2cm]
is a frame  that can be used to solve this exercise.  
Once you have a running implementation of \blue{Connect Four}, try to improve the strength of your program by
adding a non-trivial heuristic to evaluate non-terminal states.  As an example of a non-trivial heuristic you
can define a \blue{triple} as a set of three marks of either \texttt{X}s or \texttt{O}s in a row that is
followed by a blank space.  The blank space could also be between the marks.  Now if there is a state $s$ that
has $a$ triples of \texttt{X}s and $b$ triples of \texttt{O}s and the game is not finished, then define
\\[0.2cm]
\hspace*{1.3cm}
$\ds \texttt{value}(s, \texttt{X}, \texttt{limit}, \alpha, \beta) = \frac{a - b}{10}$ \quad if $\texttt{limit} = 0$.
\eox

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "artificial-intelligence"
%%% End:
